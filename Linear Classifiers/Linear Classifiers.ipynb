{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Applied Machine Learning: Assignment 4, Eric Johansson & Max Sonnelid\r\n",
    "\r\n",
    "## Implementing linear classifiers\r\n",
    "\r\n",
    "In this assignment, you will implement two algorithms to train classifiers: support vector classification and logistic regression. The pedagogical objectives of this assignment are that we should (1) get some experience of the practical considerations of implementing machine learning algorithms, (2) understand SVC and LR more thoroughly, and (3) get a taste of how a typical academic paper in machine learning looks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise question"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*When we use this set, for some strange reason our classifier performs poorly! We can't improve it by switching to a LinearSVC. Do you have an idea what's going on? Why could the classifier \"memorize\" the training data in the first case, but not in the second case?*\n",
    "\n",
    "Below the data from the assignment PM is presented in the shape of scatter plot in order to understand the data better.\n",
    "\n",
    "Studying the graph for the first training data (Gothenburg + Paris), it is easy to see that a linear classifier could be able to draw a straight line for distinguishing the two groups (red dots for rain and blue dots for sun) based on the two available features. When making predictions from the trained classifier on the same data set, the linear classifier uses this line for classifying the data points as either rain and sun. As the line divides the target features without any misplacements, the linear classifier will reach a 100 % accuracy.\n",
    "\n",
    "Instead looking at the graph for the second training data (Sydney + Paris), it is not possible to draw a straight line for distinguishing the two groups (rain resp. sun). Any attempt to draw such a line will lead to half of the data points being misplaced in the wrong groups. Both the perceptron and the linear SVC have attempted to draw such a straight line to the best of their abilities. When making predictions from these trained linear classifiers on the same data set, they both only reach an accuracy of 50 %, which is explained by the logical impossibility to create a 100 % accurate linear division of this data. For this data set, it should probably be suggested to use classifiers based on non-linear functions, for example neural networks, in order to reach an accuracy of 100 %."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scatter plots for first training data (Gothenburg + Paris)\n",
    "\n",
    "Numerical encoding of features:\n",
    "\n",
    "- Cities: Gothenburg = 1, Paris = 0\n",
    "- Months: July = 1, December = 0\n",
    "- Weather: rain = 1, sun = 0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "df = pd.DataFrame([[1, 1, 1], [1, 0, 1], [0, 1, 0], [0, 0, 1]],\r\n",
    "                columns=['City', 'Month', 'Weather'])\r\n",
    "df.plot.scatter(x='City', y='Month', c='Weather', colormap='jet', s=100)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d0143f1790>"
      ]
     },
     "metadata": {},
     "execution_count": 81
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"241.518125pt\" version=\"1.1\" viewBox=\"0 0 383.0145 241.518125\" width=\"383.0145pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 241.518125 \r\nL 383.0145 241.518125 \r\nL 383.0145 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 311.62125 228.439219 \r\nL 311.62125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M 0 5 \r\nC 1.326016 5 2.597899 4.473168 3.535534 3.535534 \r\nC 4.473168 2.597899 5 1.326016 5 0 \r\nC 5 -1.326016 4.473168 -2.597899 3.535534 -3.535534 \r\nC 2.597899 -4.473168 1.326016 -5 0 -5 \r\nC -1.326016 -5 -2.597899 -4.473168 -3.535534 -3.535534 \r\nC -4.473168 -2.597899 -5 -1.326016 -5 0 \r\nC -5 1.326016 -4.473168 2.597899 -3.535534 3.535534 \r\nC -2.597899 4.473168 -1.326016 5 0 5 \r\nz\r\n\" id=\"C0_0_a83659c6ff\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p8ab8ec0417)\">\r\n     <use style=\"fill:#800000;stroke:#800000;\" x=\"299.446705\" xlink:href=\"#C0_0_a83659c6ff\" y=\"20.882855\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p8ab8ec0417)\">\r\n     <use style=\"fill:#800000;stroke:#800000;\" x=\"299.446705\" xlink:href=\"#C0_0_a83659c6ff\" y=\"218.555582\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p8ab8ec0417)\">\r\n     <use style=\"fill:#000080;stroke:#000080;\" x=\"55.955795\" xlink:href=\"#C0_0_a83659c6ff\" y=\"20.882855\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p8ab8ec0417)\">\r\n     <use style=\"fill:#800000;stroke:#800000;\" x=\"55.955795\" xlink:href=\"#C0_0_a83659c6ff\" y=\"218.555582\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf8094d766f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.955795\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.653977\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"153.352159\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.050341\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.748523\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.446705\" xlink:href=\"#mf8094d766f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc177f5115b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"218.555582\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 222.354801)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"179.021037\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 182.820256)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"139.486491\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 143.28571)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"99.951946\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 103.751165)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"60.417401\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 64.216619)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc177f5115b\" y=\"20.882855\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 24.682074)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Month -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 24.515625 72.90625 \r\nL 43.109375 23.296875 \r\nL 61.8125 72.90625 \r\nL 76.515625 72.90625 \r\nL 76.515625 0 \r\nL 66.890625 0 \r\nL 66.890625 64.015625 \r\nL 48.09375 14.015625 \r\nL 38.1875 14.015625 \r\nL 19.390625 64.015625 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-77\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 135.390312)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-77\"/>\r\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"147.460938\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"210.839844\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"250.048828\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 311.62125 228.439219 \r\nL 311.62125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 311.62125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 311.62125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p81d27cc5f5)\" d=\"M 328.36125 228.439219 \r\nL 328.36125 227.589844 \r\nL 328.36125 11.848594 \r\nL 328.36125 10.999219 \r\nL 339.23325 10.999219 \r\nL 339.23325 11.848594 \r\nL 339.23325 227.589844 \r\nL 339.23325 228.439219 \r\nz\r\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\r\n   </g>\r\n   <image height=\"218\" id=\"imagee9b429a9c2\" transform=\"scale(1 -1)translate(0 -218)\" width=\"11\" x=\"328\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAAsAAADaCAYAAABwzrisAAAABHNCSVQICAgIfAhkiAAAARlJREFUaIHtmssOwkAMA902UPhv/hnx5tpjpijIu9qeR5Zjp9s9dJIuHyWfkA5ZViEFgU2Uzx426tJAA5qUMur+g41J+qSPgjktWwqHJgKfCJyPmcILgfMFUhurhw2T6NBumDSI4LH8W7jF5W+xbhO4+7rrdsPkhWUDpi8yCsWrCJ6XZx6OA1BeiI3j6QZsEOVlJrDqYJDzqruDjShL48gGtPA86v7BBtoNchR4eLapu73daLNuAK9kwFH3f2yY5Nz9yd/kh/5F4Cep+/YmymlWMeUtK0DMpXD+JKDKJmnUDTjS2A13n0b3A440dsN1aZgMONLYwg0u/4Mog8uX4kHSQMpXZCPPQhi4qFS+etgwUe4+DY8B0S8ZX7ncns9aJ8xmAAAAAElFTkSuQmCC\" y=\"-10\"/>\r\n   <g id=\"matplotlib.axis_3\"/>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 3.5 0 \r\n\" id=\"m88e25770f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(346.23325 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(346.23325 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(346.23325 145.262437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_10\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(346.23325 101.774437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_11\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(346.23325 58.286437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_12\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m88e25770f4\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(346.23325 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Weather -->\r\n     <defs>\r\n      <path d=\"M 3.328125 72.90625 \r\nL 13.28125 72.90625 \r\nL 28.609375 11.28125 \r\nL 43.890625 72.90625 \r\nL 54.984375 72.90625 \r\nL 70.3125 11.28125 \r\nL 85.59375 72.90625 \r\nL 95.609375 72.90625 \r\nL 77.296875 0 \r\nL 64.890625 0 \r\nL 49.515625 63.28125 \r\nL 33.984375 0 \r\nL 21.578125 0 \r\nz\r\n\" id=\"DejaVuSans-87\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     </defs>\r\n     <g transform=\"translate(373.734813 140.770781)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-87\"/>\r\n      <use x=\"93.001953\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"154.525391\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"215.804688\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"255.013672\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"318.392578\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"379.916016\" xlink:href=\"#DejaVuSans-114\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 328.36125 228.439219 \r\nL 328.36125 227.589844 \r\nL 328.36125 11.848594 \r\nL 328.36125 10.999219 \r\nL 339.23325 10.999219 \r\nL 339.23325 11.848594 \r\nL 339.23325 227.589844 \r\nL 339.23325 228.439219 \r\nz\r\n\" style=\"fill:none;stroke:#000000;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p8ab8ec0417\">\r\n   <rect height=\"217.44\" width=\"267.84\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p81d27cc5f5\">\r\n   <rect height=\"217.44\" width=\"10.872\" x=\"328.36125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scatter plots for second training set (Sydney and Paris)\n",
    "\n",
    "Numerical encoding of features:\n",
    "\n",
    "- Cities: Sydney = 1, Paris = 0\n",
    "\n",
    "- Months: July = 1, December = 0\n",
    "\n",
    "- Weather: rain = 1, sun = 0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "df2 = pd.DataFrame([[1, 1, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "                columns=['City', 'Month', 'Weather'])\n",
    "df2.plot.scatter(x='City', y='Month', c='Weather', colormap='jet', s=100)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d00f163dc0>"
      ]
     },
     "metadata": {},
     "execution_count": 82
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"241.518125pt\" version=\"1.1\" viewBox=\"0 0 383.0145 241.518125\" width=\"383.0145pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 241.518125 \r\nL 383.0145 241.518125 \r\nL 383.0145 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 311.62125 228.439219 \r\nL 311.62125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M 0 5 \r\nC 1.326016 5 2.597899 4.473168 3.535534 3.535534 \r\nC 4.473168 2.597899 5 1.326016 5 0 \r\nC 5 -1.326016 4.473168 -2.597899 3.535534 -3.535534 \r\nC 2.597899 -4.473168 1.326016 -5 0 -5 \r\nC -1.326016 -5 -2.597899 -4.473168 -3.535534 -3.535534 \r\nC -4.473168 -2.597899 -5 -1.326016 -5 0 \r\nC -5 1.326016 -4.473168 2.597899 -3.535534 3.535534 \r\nC -2.597899 4.473168 -1.326016 5 0 5 \r\nz\r\n\" id=\"C0_0_892f82c21f\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p851a8849cf)\">\r\n     <use style=\"fill:#800000;stroke:#800000;\" x=\"299.446705\" xlink:href=\"#C0_0_892f82c21f\" y=\"20.882855\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p851a8849cf)\">\r\n     <use style=\"fill:#000080;stroke:#000080;\" x=\"299.446705\" xlink:href=\"#C0_0_892f82c21f\" y=\"218.555582\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p851a8849cf)\">\r\n     <use style=\"fill:#000080;stroke:#000080;\" x=\"55.955795\" xlink:href=\"#C0_0_892f82c21f\" y=\"20.882855\"/>\r\n    </g>\r\n    <g clip-path=\"url(#p851a8849cf)\">\r\n     <use style=\"fill:#800000;stroke:#800000;\" x=\"55.955795\" xlink:href=\"#C0_0_892f82c21f\" y=\"218.555582\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7d5388cf60\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.955795\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.653977\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"153.352159\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.050341\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.748523\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.446705\" xlink:href=\"#m7d5388cf60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m247addbc2a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"218.555582\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 222.354801)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"179.021037\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 182.820256)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"139.486491\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 143.28571)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"99.951946\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 103.751165)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"60.417401\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 64.216619)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m247addbc2a\" y=\"20.882855\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 24.682074)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Month -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 24.515625 72.90625 \r\nL 43.109375 23.296875 \r\nL 61.8125 72.90625 \r\nL 76.515625 72.90625 \r\nL 76.515625 0 \r\nL 66.890625 0 \r\nL 66.890625 64.015625 \r\nL 48.09375 14.015625 \r\nL 38.1875 14.015625 \r\nL 19.390625 64.015625 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-77\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 135.390312)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-77\"/>\r\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"147.460938\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"210.839844\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"250.048828\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 311.62125 228.439219 \r\nL 311.62125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 311.62125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 311.62125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p627cc1daa1)\" d=\"M 328.36125 228.439219 \r\nL 328.36125 227.589844 \r\nL 328.36125 11.848594 \r\nL 328.36125 10.999219 \r\nL 339.23325 10.999219 \r\nL 339.23325 11.848594 \r\nL 339.23325 227.589844 \r\nL 339.23325 228.439219 \r\nz\r\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\r\n   </g>\r\n   <image height=\"218\" id=\"imageb9fc094557\" transform=\"scale(1 -1)translate(0 -218)\" width=\"11\" x=\"328\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAAsAAADaCAYAAABwzrisAAAABHNCSVQICAgIfAhkiAAAARlJREFUaIHtmssOwkAMA902UPhv/hnx5tpjpijIu9qeR5Zjp9s9dJIuHyWfkA5ZViEFgU2Uzx426tJAA5qUMur+g41J+qSPgjktWwqHJgKfCJyPmcILgfMFUhurhw2T6NBumDSI4LH8W7jF5W+xbhO4+7rrdsPkhWUDpi8yCsWrCJ6XZx6OA1BeiI3j6QZsEOVlJrDqYJDzqruDjShL48gGtPA86v7BBtoNchR4eLapu73daLNuAK9kwFH3f2yY5Nz9yd/kh/5F4Cep+/YmymlWMeUtK0DMpXD+JKDKJmnUDTjS2A13n0b3A440dsN1aZgMONLYwg0u/4Mog8uX4kHSQMpXZCPPQhi4qFS+etgwUe4+DY8B0S8ZX7ncns9aJ8xmAAAAAElFTkSuQmCC\" y=\"-10\"/>\r\n   <g id=\"matplotlib.axis_3\"/>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 3.5 0 \r\n\" id=\"m3c9ae80ab9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(346.23325 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(346.23325 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(346.23325 145.262437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_10\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(346.23325 101.774437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_11\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(346.23325 58.286437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_12\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.23325\" xlink:href=\"#m3c9ae80ab9\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(346.23325 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Weather -->\r\n     <defs>\r\n      <path d=\"M 3.328125 72.90625 \r\nL 13.28125 72.90625 \r\nL 28.609375 11.28125 \r\nL 43.890625 72.90625 \r\nL 54.984375 72.90625 \r\nL 70.3125 11.28125 \r\nL 85.59375 72.90625 \r\nL 95.609375 72.90625 \r\nL 77.296875 0 \r\nL 64.890625 0 \r\nL 49.515625 63.28125 \r\nL 33.984375 0 \r\nL 21.578125 0 \r\nz\r\n\" id=\"DejaVuSans-87\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     </defs>\r\n     <g transform=\"translate(373.734813 140.770781)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-87\"/>\r\n      <use x=\"93.001953\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"154.525391\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"215.804688\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"255.013672\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"318.392578\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"379.916016\" xlink:href=\"#DejaVuSans-114\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 328.36125 228.439219 \r\nL 328.36125 227.589844 \r\nL 328.36125 11.848594 \r\nL 328.36125 10.999219 \r\nL 339.23325 10.999219 \r\nL 339.23325 11.848594 \r\nL 339.23325 227.589844 \r\nL 339.23325 228.439219 \r\nz\r\n\" style=\"fill:none;stroke:#000000;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p851a8849cf\">\r\n   <rect height=\"217.44\" width=\"267.84\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p627cc1daa1\">\r\n   <rect height=\"217.44\" width=\"10.872\" x=\"328.36125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVC classifier class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.base import BaseEstimator\n",
    "import scipy.linalg.blas as blas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General class for binary linear classifiers (given code)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Given code from doc_classification for running the classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels.\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Read all the documents.\n",
    "    X, Y = read_data('pa4/data/all_sentiment_shuffled.txt')\n",
    "\n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        #SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        Pegasos_SVC()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training time: 54.88 sec.\n",
      "Accuracy: 0.8070.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pegasos_SVC\n",
    "\n",
    "Below the Pegasos algorithm for training support vector classifiers is implemented as described in the clarification document. Several values for the regularization lamda were tried, e.g. 1.0 gave an accuracy score of around 0.75, but 0.001 gave an accuracy score above 0.8 and therefore 0.001 was chosen as the default value for the regularization lamda. In the example code about the Perceptron, 20 was set as the default value for the number of iterations. As 20 gave an accuracy above 0.8 also for Pegasos_SVC, this value was kept as the default value for the number of iterations. Probably the accuracy score can be improved even further by using methods such as GridSearchCV in order to more systematically iterate through different values for the regularization lamda and the number of iterations. However, this was not done in this assignment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Pegasos algorithm:\n",
    "        for i in range(0, self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if y*score < 1:\n",
    "                    self.w = (\n",
    "                        1 - eta*self.lambda_reg)*self.w + eta * y * x\n",
    "                else:\n",
    "                    self.w = (1 - eta*self.lambda_reg) * self.w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result with regular Pegasos algorithm.\n",
    "           \n",
    "Training time: 10.62 sec.\n",
    "\n",
    "Accuracy: 0.8191.\n",
    "\n",
    "Lambda: 0.001\n",
    "\n",
    "### Without SelectKBest(k=1000)\n",
    "Training time: 54.88 sec.\n",
    "\n",
    "Accuracy: 0.8070.\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below the Logicstic Regression algorithm for classifiers is implemented as described in the clarification document. Similarly as with Pegasos_SVC, 0.001 was set as the regularization lambda and 20 as the number of iterations, which resulted in an accuracy score above 0.8.\n",
    "\n",
    "Notably, Logistic Regression gives a slightly lower accuracy score compared to Pegasos_SVC (0.8082 vs 0.8191). The only difference between these two classifiers is that Pegasos_SVC uses hinge loss, while Logistic Regression uses log loss. The advantage with log loss is that its weight vector is constantly evolving by accumulating information from data, which leads to better estimates for probabilities at the cost of accuracy as misclassification is not punished in any distinct way. Hinge loss, on the other side, punishes misclassification clearly as the weight vector is only updated for the misclassified data points, which leads to a better accuracy at the cost of less accurate probabilities. This is shown in our case by the fact that Logistic Regression has a slightly lower accuracy than Pegasos_SVC.\n",
    "\n",
    "Considering the training time, no noticeable difference was found when running both classifiers with feature limitation (SelectKBest(k=1000)). However, when removing this limitation, Pegasos_SVC was considerably faster compared to Logistic_Regression. The accuracy for both Pegasos_SVC and Linear_Regression decreased slightly when running them without the feature limitations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Logistic_Regression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Logistic Regression algorithm:\n",
    "        for i in range(1, self.n_iter+1):\n",
    "            t=0\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t+1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights by the log loss algorithm.\n",
    "                self.w = (1-eta*self.lambda_reg)*self.w + \\\n",
    "                    (y / (1+math.exp(y*score))) * eta * x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded result with Logistic Regression\n",
    "\n",
    "Training time: 11.14 sec.\n",
    "\n",
    "Accuracy: 0.8082.\n",
    "        \n",
    "Lambda: 0.001\n",
    "\n",
    "### Without SelectKBest(k=1000)\n",
    "\n",
    "Training time: 84.77 sec\n",
    "\n",
    "Accuracy: 0.8019\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BONUS TASK 1: Making your code more efficient"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BONUS TASK 1A: Faster linear algebra operations\n",
    "\n",
    "In this bonus task, it was attempted to speed up both of the algorithms by implementing the following functions from the scipy.linalg.blas library: ddot(x, y), dscal(a, x) &  daxpy(x, y, a=a). The aim with this is to improve the speed in the dot product and scalar product operations. When using normal normal NumPy linear algebra operations, a number of safety checks are carried out by NumPy, which can be avoided by the BLAS functions. Furthermore, BLAS methods often take advantage of CPU-specific operations depending on the current machine and is able to let kernels run in parallel for speeding up calculations. It was only in the fit method that calculations were found that could be speeded up and there the BLAS functions were implemented in all suitable operations.\n",
    "\n",
    "Looking at the results for the Pegasos_SVC_faster, it was possible to reduce the running time with 25 % compared to Pegasos_SVC (from 10.62s to 7.94s). Looking at the result for the Logistic_Regression_faster, it was possible to reduce the running time with 16 % compared to Logistic_Regression (from 11.14s to 9.36s). For these rather short training times, the absolute reduction in training time is not rather impressive. However, imaging starting with a training time of 1 hour. Being able to reduce that training time with 20 % implies an absolute reduction of the training time by 12 minutes, which could be very helpful in a practical situation. These results were only produced by one single algorithm run and a more fair comparison of the training time could probably be done by running each algorithm several times and then compare the mean values from the iterations. This because the training time differs caused by random factors for each iteration. However, this was not done for this task because of time deficiency. Finally, no change in accuracy score was registered for both of the classifiers as expected, which means that it is possible to decrease the training time without compromising on accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pegasos_SVC_faster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Pegasos_SVC_faster(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Pegasos algorithm (with added BLAS methods):\n",
    "        for i in range(0, self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (blas.dscal(self.lambda_reg, t))\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = blas.ddot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if blas.dscal(y, score) < 1:\n",
    "                    blas.daxpy(x, blas.dscal(\n",
    "                        (1-blas.dscal(eta, self.lambda_reg)), self.w), a=blas.dscal(eta, y))\n",
    "                else:\n",
    "                    blas.dscal((1-blas.dscal(eta, self.lambda_reg)), self.w)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded result with the fast Pegasos algorithm.\n",
    "           \n",
    "Training time: 7.94 sec.\n",
    "\n",
    "Accuracy: 0.8191.\n",
    "\n",
    "Lambda: 0.001           "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic_Regression_faster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Logistic_Regression_faster(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Logistic Regression algorithm (with added BLAS methods):\n",
    "        for i in range(0, self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t = t+1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (blas.dscal(self.lambda_reg, t))\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = blas.ddot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights by the log loss method\n",
    "\n",
    "                blas.daxpy(x, blas.dscal(\n",
    "                    (1-blas.dscal(eta, self.lambda_reg)), self.w), a=(blas.dscal(eta, (y / (1+math.exp(blas.dscal(y, score)))))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded result with fast Logistic Regression algorithm.\n",
    "\n",
    "Training time: 9.36 sec.\n",
    "\n",
    "Accuracy: 0.8082.\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BONUS TASK 1B: Using sparse vectors\n",
    "\n",
    "In this bonus task, efficient methods for handling sparse vectors were implemented for both of the classifiers, which improve the use of memory and thus the speed when running the algorithm without the feature limitation (without SelectKBest(k=1000)). The SparsePerceptron code was used as example for the implementation, which resulted in implementing sparse_dense_dot() when calculating the score and add_sparse_to_dense() when calculating the weight vector.\n",
    "\n",
    "A sparse vector is a vector with a large number of zeroes that take up unneccessary space, which can be expected to be the case for our data after removing the feature limitation. Therefore, sparse_dense_dot() and add_sparse_to_dense() work by only saving the indices and value for each weight connected to a feature, and thus save space by not needing to save zero values.\n",
    "\n",
    "Looking at the result for the Sparse_Pegasos_SVC, it was possible to reduce the running time with 45 % compared to Pegasos_SVC (from 54.88s to 30.23s). Looking at the result for the Sparse_Logistic_Regression, it was possible to reduce the running time with 64 % compared to Logistic_Regression (from 84.77s to 30.91s). Compared to the improvements made in task 1A, the improvements made in this sub-task are bigger. Apparently, it was possible to save some memory during the calculations by not needing to store the zero values in the weight vector. However, as the data set in task 1A respectively in task 1B are not comparable because of the difference in number of features, these improvements can not be compared as equals."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Help functions for task 1B\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# The following part is for the optional task.\n",
    "\n",
    "# Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "# Here are two utility functions that help us carry out some vector\n",
    "# operations that we'll need.\n",
    "\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Pegasos_SVC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class Sparse_Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                self.w = (1 - eta*self.lambda_reg)*self.w\n",
    "\n",
    "                # If there was an error, update the weights by the hinge loss algorithm.\n",
    "                if y*score < 1:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, (eta*y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Sparse_Pegasos_SVC\n",
    "\n",
    "Training time: 30.23 sec\n",
    "\n",
    "Accuracy: 0.8070\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Logistic_Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class Sparse_Logistic_Regression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                self.w = (1 - eta*self.lambda_reg)*self.w\n",
    "\n",
    "                # Update weights by the log loss algorithm\n",
    "                # (This corresponds to self.w += y*x above.)\n",
    "                add_sparse_to_dense(\n",
    "                    x, self.w, (eta*(y / (1+math.exp(y*score)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Sparse_Logistic_Regression\n",
    "\n",
    "Training time: 30.91 sec\n",
    "\n",
    "Accuracy: 0.8019\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BONUS TASK 1C: Speeding up the scaling operation\n",
    "\n",
    "For this bonus task, it was chosen to have Sparse_Pegasos_SVC and Sparse_Logistic_Regression as starting points for the new changes implemented. In the previous methods, the weight vector has been scaled for every single iteration. However, this is in fact not necessary as it is possible to only update the scaling factor in every iteration and then multiply the scaling factor with the weight vector only after all iterations have run. Such a change simplifies the calculations made and will thus result in a shorter training time.\n",
    "\n",
    "Looking at the result for the Sparse_Pegasos_SVC_faster, it was possible to reduce the running time with 36 % compared to Sparse_Pegasos_SVC (from 30.23s to 19.28s). Looking at the result for the Sparse_Logistic_Regression_faster, it was possible to reduce the running time with 40 % compared to Sparse_Logistic_Regression (from 30.91s to 18.22s)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Pegasos_SVC_faster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class Sparse_Pegasos_SVC_faster(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        a = 1\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                score = score * a\n",
    "\n",
    "                # Update the scaling factor\n",
    "                a = (1 - eta*self.lambda_reg)*a\n",
    "\n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if y*score < 1:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, ((eta/a)*y))\n",
    "\n",
    "        # Multiply the weight vector by the scaling factor\n",
    "        self.w = a * self.w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Pegasos_SVC_faster\n",
    "\n",
    "Training time: 19.28 sec\n",
    "\n",
    "Accuracy: 0.8070\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Logistic_Regression_faster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Sparse_Logistic_Regression_faster(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        a = 1\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (self.lambda_reg*t)\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "                score = score * a\n",
    "\n",
    "                # Update the scaling factor\n",
    "                a = (1 - eta*self.lambda_reg)*a\n",
    "\n",
    "                # Update weight by the log loss algorithm\n",
    "                # (This corresponds to self.w += y*x above.)\n",
    "                add_sparse_to_dense(\n",
    "                    x, self.w, (eta*(y / (a*(1+math.exp(y*score))))))\n",
    "\n",
    "        # Multiply the weight vector by the scaling factor\n",
    "        self.w = a * self.w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Sparse_Logistic_Regression_faster\n",
    "\n",
    "Training time: 18.22 sec\n",
    "\n",
    "Accuracy: 0.8019\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRA WORK: Combining task 1A, 1B and 1C\n",
    "\n",
    "It was not clearly stated in the assignment PM whether it was needed to combine the improvements by the BLAS methods with the improvements made by sparse vectors and scaling operations. However, it was interesting to find out much the training time could be decreased if all improvement would be combined, especially compared to the training times for the first Pegasos_SVC and Logistic_Regression.\n",
    "\n",
    "Looking at the result for the Sparse_Pegasos_SVC_fastest, it was possible to reduce the running time with 30 % compared to Sparse_Pegasos_SVC_faster (from 19.28s to 13.40s) and 76 % compared to Pegasos_SVC (from 54.88s to 13.40s). Looking at the result for the Sparse_Logistic_Regression_fastest, it was possible to reduce the running time with 8 % compared to Sparse_Logistic_Regression_faster (from 18.22s to 16.71s) and 81 % compared to Logistic_Regression (from 88.77s to 16.71s).\n",
    "\n",
    "Notably, the accuracy for Sparse_Pegasos_SVC_fastest decreased slightly compared to Sparse_Pegasos_SVC_faster and probably some small mistake has been done in this implementation. However, as the decrease is very small, it was not attempted to improve the accuracy. It is very fascinating to be able to decrease the running time for the algorithm to such a big extent only by implementing efficient calculation methods and this skill is something that the report authors will have great use for in future projects."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Pegasos_SVC_fastest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class Sparse_Pegasos_SVC_fastest(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        a = 1\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (blas.dscal(self.lambda_reg, t))\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                blas.dscal(a, score)\n",
    "\n",
    "                # Update the scaling factor\n",
    "                a = blas.dscal((1 - blas.dscal(eta, self.lambda_reg)), a)\n",
    "\n",
    "                # If there was an error, update the weights by the hinge loss algorithm\n",
    "                if blas.dscal(y, score) < 1:\n",
    "                    add_sparse_to_dense(x, self.w, blas.dscal(\n",
    "                        blas.dscal(eta, (1/a)), y))\n",
    "        \n",
    "        # Multiply the weight vector by the scaling factor\n",
    "        blas.dscal(a, self.w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Sparse_Pegasos_SVC_fastest\n",
    "\n",
    "Training time: 13.40 sec\n",
    "\n",
    "Accuracy: 0.8015\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sparse_Logistic_Regression_fastest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "class Sparse_Logistic_Regression_fastest(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20, lambda_reg=0.001):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        a = 1\n",
    "        for i in range(self.n_iter):\n",
    "            t = 1\n",
    "            for x, y in XY:\n",
    "\n",
    "                t = t + 1\n",
    "\n",
    "                # Compute the learning rate\n",
    "                eta = 1 / (blas.dscal(self.lambda_reg, t))\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                score = blas.dscal(a, score)\n",
    "\n",
    "                # Uopdate the scaling factor\n",
    "                a = blas.dscal((1 - blas.dscal(eta, self.lambda_reg)), a)\n",
    "\n",
    "                # Update the weights by the log loss algorithm\n",
    "                # (This corresponds to self.w += y*x above.)\n",
    "                add_sparse_to_dense(\n",
    "                    x, self.w, (blas.dscal(eta, blas.dscal(y, (1 / (blas.dscal(a, (1+math.exp(blas.dscal(y, score))))))))))\n",
    "\n",
    "        # Multiply the weight vector by the scaling factor\n",
    "        blas.dscal(a, self.w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recorded results for Sparse_Logistic_Regression_fastest\n",
    "\n",
    "Training time: 16.71 sec\n",
    "\n",
    "Accuracy: 0.8019\n",
    "\n",
    "Lambda: 0.001"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}